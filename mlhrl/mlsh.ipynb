{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta learning shared hierarchies\n",
    "\n",
    "Here is the [link](https://arxiv.org/abs/1710.09767) to the original paper\n",
    "\n",
    "Following the OpenAI implementation \n",
    "\n",
    "* but we don't use MPI for clearer understanding\n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "* set up the PYTHONPATH as specified in the [OpenAI code](https://github.com/openai/mlsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules .... \n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "\n",
    "# OpenAI baselines implementation\n",
    "from distributions import make_pdtype\n",
    "import tf_util as U\n",
    "import misc_util as M\n",
    "import logger as L\n",
    "import dataset as D\n",
    "\n",
    "# meta-learning original code testing environment\n",
    "import test_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"/tmp/mlsh_log\"\n",
    "CKPT_DIR = \"/tmp/mlsh_ckpt\"\n",
    "\n",
    "NUM_TIMESTEPS = 1e9\n",
    "M_SEED = 1401\n",
    "\n",
    "# NOTE: CHANGE THE BELOW AS NEEDED\n",
    "NUM_GPU = 0  \n",
    "GLOBAL_STEP_DEVICE = '/cpu:0'\n",
    "DEVICE_PREFIX = '/device:' # probably only used when GPU is needed\n",
    "LOG_DEVICE_PLACEMENT = True\n",
    "\n",
    "# test envs built by OpenAI\n",
    "gym_env_name = 'MovementBandits-v0' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = multiprocessing.cpu_count()\n",
    "# well decide whether you have hyper threading\n",
    "n_cpu = n_cpu // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Hierarchical RL specific\n",
    "num_subpolicies = 2 # How do we decide number of subpolicy ? \n",
    "pri_duration = 1000 # How do we decide duration number ?\n",
    "num_rollouts = 2000 # How do we decide number of rollouts ?\n",
    "num_batches = 15\n",
    "warmup_time = 20 \n",
    "train_time = 30\n",
    "replay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "tf_config=tf.ConfigProto(log_device_placement=True,\n",
    "                         allow_soft_placement=True,\n",
    "                         intra_op_parallelism_threads=n_cpu,\n",
    "                         inter_op_parallelism_threads=n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"networks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we are going to implement the policy for our master action\n",
    "\n",
    "Each policy parameters will be optimized based on [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    \"\"\"\n",
    "    At the moment only used MLP policy. (can switch to cnn when we need to)\n",
    "    Would love to try a bayseian policy in here. (whats the benefit?)\n",
    "    \n",
    "    Args: \n",
    "        name - for the tf variable scope\n",
    "        observation - tf placeholder observation tensor\n",
    "        action_space - gym env action_space for the particular action\n",
    "        hidden_size - fc layer hidden units\n",
    "        num_hidden_layers - layers\n",
    "        num_subpolicies - specify sub policies number \n",
    "        gaussian_fixed_var\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 name, \n",
    "                 observation, \n",
    "                 action_space,\n",
    "                 hidden_size, # Fully-connected Layer hidden-layer units\n",
    "                 num_hidden_layers,\n",
    "                 num_subpolicies,\n",
    "                 gaussian_fixed_var=True\n",
    "                ):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_subpolicies = num_subpolicies\n",
    "        self.gaussian_fixed_var = gaussian_fixed_var\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self.namescope=tf.get_variable_scope().name\n",
    "            \n",
    "            # NOTE: in the original code, \n",
    "            # they normalize the observation to zero mean however why calculate running mean and running std.\n",
    "            # TODO: observation update running mean and std.\n",
    "            \n",
    "            # build network for value function\n",
    "            last_output = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_output = tf.layers.dense(last_output, \n",
    "                                              hidden_size, \n",
    "                                              name='vfn_fc%i'%(i+1), \n",
    "                                              activation=tf.nn.tanh)\n",
    "            # estimate expected values\n",
    "            self.value_pred = tf.layers.dense(last_output, 1, name='vfn_final')\n",
    "            \n",
    "            # build network for master policy to optimize against\n",
    "            # which subpolicy to pick, as master works by choosing subpolicy\n",
    "            last_output = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_output = tf.layers.dense(last_output, \n",
    "                                              hidden_size, \n",
    "                                              name='master_%i'%(i+1), \n",
    "                                              activation=tf.nn.tanh)\n",
    "            # pick subpolicy\n",
    "            self.policy_selector_prob = tf.layers.dense(last_output, \n",
    "                                                        num_subpolicies, \n",
    "                                                        name='master_final')\n",
    "            # make probability distribution\n",
    "            pdtype = make_pdtype(action_space)\n",
    "            self.pdtype = pdtype\n",
    "            self.pd = pdtype.pdfromflat(self.policy_selector_prob)\n",
    "            \n",
    "        # sample actions\n",
    "        stochastic = tf.placeholder(dtype=tf.bool, shape=())\n",
    "        action = tf.cond(stochastic, lambda: self.pd.sample(), lambda: self.pd.mode())\n",
    "        # i.e. function([placeholders], [outputs])\n",
    "        self._act = U.function([stochastic, observation], \n",
    "                               [action, self.value_pred])\n",
    "            \n",
    "        # debug\n",
    "        self._debug = U.function([stochastic, observation], \n",
    "                                 [action, self.policy_selector_prob])\n",
    "        \n",
    "    def act(self, stochastic, observation):\n",
    "        # no need for observation ?\n",
    "        act1, v_pred1 = self._act(stochastic, observation[None])\n",
    "        return act1[0], v_pred1[0]\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.MODEL_VARIABLES, self.namescope)\n",
    "    \n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.namescope)\n",
    "    \n",
    "    def reset(self):\n",
    "        with tf.variable_scope(self.namescope, reuse=True):\n",
    "            trainable_vars = self.get_trainable_variables()\n",
    "            initalizer = tf.variables_initializer(trainable_vars)\n",
    "            tf.get_default_session().run(initalizer)\n",
    "    \n",
    "    # debug\n",
    "    \n",
    "    def debug(self, stochastic, observation):\n",
    "        \"\"\"\n",
    "        check which selection we have got\n",
    "        \"\"\"\n",
    "        _, selection = self._debug(stochastic, observation[None])\n",
    "        return selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPolicy(object):\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 observation,\n",
    "                 action_space,\n",
    "                 hidden_size,\n",
    "                 num_hidden_layers,\n",
    "                 gaussian_fixed_var=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.gaussian_fixed_var = gaussian_fixed_var\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.namescope = tf.get_variable_scope().name\n",
    "            \n",
    "            # NOTE: in the original code, \n",
    "            # they normalize the observation to zero mean \n",
    "            # however why calculate running mean and running std.\n",
    "            \n",
    "            #value function\n",
    "            last_out = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_out = tf.layers.dense(last_out, hidden_size, activation=tf.nn.tanh, name='vfn_%i'%(i+1))\n",
    "            self.value_pred = tf.layers.dense(last_out, 1, name='vfn_final')\n",
    "            \n",
    "            # sub policy fn\n",
    "            pdtype = make_pdtype(action_space)\n",
    "            paramshape = pdtype.param_shape()[0]\n",
    "            self.pdtype = pdtype\n",
    "            last_out = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_out = tf.layers.dense(last_out, \n",
    "                                           hidden_size, \n",
    "                                           activation=tf.nn.tanh, \n",
    "                                           name='policy_%i'%(i+1))\n",
    "            if gaussian_fixed_var and isinstance(action_space, gym.spaces.Box):\n",
    "                mean = tf.layers.dense(last_out, \n",
    "                                       paramshape//2, \n",
    "                                       name='policy_final')\n",
    "                logstd = tf.get_variable(name='logstd', \n",
    "                                         shape=[1, paramshape//2], \n",
    "                                         initializer=tf.zeros_initializer())\n",
    "                pdparam = tf.concat([mean, mean * 0.0 + logstd], axis=1)\n",
    "            else:\n",
    "                pdparam = tf.layers.dense(last_out, \n",
    "                                          paramshape, \n",
    "                                          name='policy_final')\n",
    "        \n",
    "            self.pd = pdtype.pdfromflat(pdparam)\n",
    "            \n",
    "            #sample actions\n",
    "            stochastic = tf.placeholder(dtype=tf.bool, shape=())\n",
    "            action = tf.cond(stochastic, lambda: self.pd.sample(), lambda: self.pd.mode())\n",
    "            # i.e. function([placeholders], [outputs])\n",
    "            self._act = U.function([stochastic, observation], \n",
    "                                   [action, self.value_pred])\n",
    "            \n",
    "    def act(self, stochastic, observation):\n",
    "        act1, v_pred1 = self._act(stochastic, observation[None])\n",
    "        return act1[0], v_pred1[0]\n",
    "    def get_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.MODEL_VARIABLES, self.namescope)\n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.namescope)\n",
    "    def reset(self):\n",
    "        with tf.variable_scope(self.namescope, reuse=True):\n",
    "            train_vars = self.get_trainable_variables()\n",
    "            initializer = tf.variables_initializer(train_vars)\n",
    "            tf.get_default_session().run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 master_pol,\n",
    "                 master_oldpol,\n",
    "                 subpolicies,\n",
    "                 old_subpolicies,\n",
    "                 clip_param=0.2,\n",
    "                 entropy_coeff=0,\n",
    "                 optim_epochs=10,\n",
    "                 optim_stepsize=3e-4,\n",
    "                 optim_batchsize=64):\n",
    "        self.master_policy = master_pol\n",
    "        self.clip_param = clip_param\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.optim_epochs = optim_epochs\n",
    "        self.optim_stepsize = optim_stepsize\n",
    "        self.optim_batchsize = optim_batchsize\n",
    "        self.num_subpolicies = len(subpolicies)\n",
    "        self.subpolicies = subpolicies\n",
    "        obs_space = env.observation_space\n",
    "        act_space = env.action_space\n",
    "        \n",
    "        # train theta (Master)\n",
    "        self.master_observation = U.get_placeholder_cached(name='observation') # look for the observation in default graph\n",
    "        self.master_action = self.master_policy.pdtype.sample_placeholder([None]) # create a placeholder for sampling with shape as None in the distribution\n",
    "        self.master_adv = tf.placeholder(dtype=tf.float32, shape=[None]) # target advantage function\n",
    "        self.master_emp_return = tf.placeholder(dtype=tf.float32, shape=[None]) # empirical return\n",
    "        total_loss = self.policy_loss(master_pol, \n",
    "                                      master_oldpol, \n",
    "                                      observation, \n",
    "                                      action, \n",
    "                                      adv, \n",
    "                                      emp_return, \n",
    "                                      clip_param)\n",
    "        \n",
    "        self.master_policy_vars_list = master_pol.get_trainable_variables()\n",
    "        # feed [obs, action, adv, emp_return], get flatgrad(total_loss, vars_list)\n",
    "        master_grads = tf.gradients(total_loss, self.master_policy_vars_list)\n",
    "        #flat_master_grads = U.flatgrad(total_loss, self.master_policy_vars_list)\n",
    "        #self.master_loss = U.function([observation, action, adv, emp_return], flat_master_grads)\n",
    "        master_grads = [list(zip(g, p)) for g, p in zip(master_grads, self.master_policy_vars_list)]\n",
    "        # we are not using MPI.\n",
    "        self.master_adam = tf.train.AdamOptimizer(name='master_adam')\n",
    "        self.master_train_op = self.master_adam.apply_gradients(master_grads)\n",
    "        \n",
    "        self.assign_oldpol_equal_new = U.function([], [], updates=[tf.assign(old_var, new_var)\n",
    "            for (old_var, new_var) in M.zipsame(master_oldpol.get_variables(), master_pol.get_variables())])\n",
    "        \n",
    "        \n",
    "        # sub policies\n",
    "        self.assign_subpols = []\n",
    "        self.change_subpols = []\n",
    "        self.subpols_adam = []\n",
    "        self.subpols_losses = []\n",
    "        self.subpols_action = subpolicies[0].pdtype.sample_placeholder([None])\n",
    "        self.subpols_train_op = []\n",
    "        \n",
    "        for i in range(self.num_subpolicies):\n",
    "            vars_list = subpolicies[i].get_trainable_variables()\n",
    "            self.subpols_adam.append(tf.train.AdamOptimizer(name='subpol_%i_adam' % i))\n",
    "            \n",
    "            # loss for test\n",
    "            loss = self.policy_loss(subpolicies[i], \n",
    "                                    old_subpolicies[i], \n",
    "                                    observation, \n",
    "                                    self.subpols_action,\n",
    "                                    adv,\n",
    "                                    emp_return,\n",
    "                                    clip_param)\n",
    "            self.assign_subpols.append(U.function([], [], updates=[tf.assign(oldv, newv)\n",
    "                for (oldv, newv) in M.zipsame(old_subpolicies[i].get_variables(), subpolicies[i].get_variables())]))\n",
    "            self.zerograd = U.function([], self.nograd(vars_list))\n",
    "        \n",
    "        U.initialize()\n",
    "    \n",
    "    \n",
    "    def nograd(self, vars_list):\n",
    "        \"\"\"return zeros given the vars list\"\"\"\n",
    "        return tf.concat(axis=0, values=[\n",
    "            tf.reshape(tf.zeros_like(v), [U.numel(v)])\n",
    "            for v in vars_list\n",
    "        ])\n",
    "        \n",
    "    def policy_loss(self, pi, old_pi, observation, action, adv, emp_return, clip_param):\n",
    "        \"\"\"From the mlsh implementation, differ from baselines/PPO\"\"\"\n",
    "        \n",
    "        old_pi_clipped = tf.clip_by_value(old_pi.pd.logp(action), -20, 20) # the softmax entropy of the action\n",
    "        ratio = tf.exp(pi.pd.logp(action) - old_pi_clipped) \n",
    "        surr1 = ratio * adv # surrogate from conservative policy iteration\n",
    "        surr2 = tf.clip_by_value(ratio, 1.0 - clip_param, 1.0 + clip_param) * adv\n",
    "        policy_surrogate = - tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "        \n",
    "        # this part is different in comparison to openai baseline ppo1.\n",
    "        vf_loss1 = tf.square(pi.value_pred - emp_return)\n",
    "        value_pred_clipped = old_pi.value_pred + tf.clip_by_value(pi.value_pred - old_pi.value_pred, \n",
    "                                                                  -clip_param, \n",
    "                                                                  clip_param)\n",
    "        vf_loss2 = tf.square(value_pred_clipped - emp_return)\n",
    "        vf_loss = 0.5 * tf.reduce_mean(tf.maximum(vf_loss1, vf_loss2))\n",
    "        total_loss = policy_surrogate + vf_loss\n",
    "        return total_loss\n",
    "    \n",
    "    def update_master_policy(self, seg):\n",
    "        observation = seg[\"macro_observation\"]\n",
    "        adv = seg[\"macro_adv\"]\n",
    "        action = seg[\"macro_action\"]\n",
    "        td_lambda_return= seg[\"macro_tdlamret\"]\n",
    "        \n",
    "        # we are not using MPI here, so just do it as usual.\n",
    "        # otherwise we should average out the adv, with mean and std.\n",
    "        \n",
    "        data = D.Dataset(dict(ob=observation, ac=action, adv=adv, vtarg=td_lambda_return), shuffle=True)\n",
    "        optim_batchsize = min(self.optim_batchsize, observation.shape[0])\n",
    "        \n",
    "        # TODO: observation update running mean and std.\n",
    "        \n",
    "        # master only\n",
    "        self.assign_oldpol_equal_new()\n",
    "        \n",
    "        for _ in range(self.optim_epochs):\n",
    "            for batch in data.iterate_once(optim_batchsize):\n",
    "                feed_dict = {}\n",
    "                feed_dict[self.master_observation] = batch['ob']\n",
    "                feed_dict[self.master_action] = batch['ac']\n",
    "                feed_dict[self.master_adv] = batch['adv']\n",
    "                feed_dict[self.master_emp_return] = batch['vtarg'] #critic\n",
    "                \n",
    "                # feed to policy\n",
    "                _ = U.get_session().run([self.master_train_op], feed_dict)\n",
    "        \n",
    "        ep_rets = flatten_lists(seg[\"ep_rets\"])\n",
    "        ep_rets = flatten_lists(ep_rets)\n",
    "        ep_lens = flatten_lists(seg[\"ep_lens\"])\n",
    "        ep_lens = flatten_lists(ep_lens)\n",
    "        \n",
    "        L.Logger.logkv('Mean episode return', np.mean(ep_rets))\n",
    "        L.Logger.logkv('Mean episode length', np.mean(ep_lens))\n",
    "        L.Logger.dumpkvs()\n",
    "        \n",
    "    def update_sub_policies(self, test_segs, num_batches, optimize=True):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.Logger.DEFAULT = L.Logger.CURRENT = L.Logger(dir=None, \n",
    "                                               output_formats=[L.HumanOutputFormat(sys.stdout),\n",
    "                                                               L.CSVOutputFormat(os.path.join(LOG_DIR, 'log.csv'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-21 21:33:06,194] Making new env: MovementBandits-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeded\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "tf.gfile.MakeDirs(LOG_DIR)\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "tf.Session(config=tf_config, graph=g).__enter__()\n",
    "\n",
    "env = gym.make(gym_env_name) \n",
    "observ_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "observ_holder = U.get_placeholder(name='observation', \n",
    "                                  dtype=tf.float32, \n",
    "                                  shape=[None, observ_space.shape[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO master policies\n",
    "policy = Policy(name='master_policy', \n",
    "                observation=observ_holder, \n",
    "                action_space=action_space, \n",
    "                hidden_size=32, \n",
    "                num_hidden_layers=2,\n",
    "                num_subpolicies=num_subpolicies)\n",
    "old_policy = Policy(name='old_master_policy',\n",
    "                    observation=observ_holder,\n",
    "                    action_space=action_space,\n",
    "                    hidden_size=32,\n",
    "                    num_hidden_layers=2,\n",
    "                    num_subpolicies=num_subpolicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO subpolicies\n",
    "subpolicies = []\n",
    "old_subpolicies = []\n",
    "for i in range(num_subpolicies):\n",
    "    subpolicies.append(SubPolicy(name='subpolicy_%i' % i,\n",
    "                                  observation=observ_holder,\n",
    "                                  action_space=action_space,\n",
    "                                  hidden_size=32,\n",
    "                                  num_hidden_layers=2))\n",
    "    old_subpolicies.append(SubPolicy(name='old_subpolicy_%i' % i, \n",
    "                                     observation=observ_holder,\n",
    "                                     action_space=action_space,\n",
    "                                     hidden_size=32,\n",
    "                                     num_hidden_layers=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(2659,), dtype=float32)\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-21 21:33:09,670] VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n"
     ]
    }
   ],
   "source": [
    "# create learner (i.e. agent)\n",
    "learner = Learner(env, policy, old_policy, subpolicies, old_subpolicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
