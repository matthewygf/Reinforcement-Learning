{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta learning shared hierarchies\n",
    "\n",
    "Here is the [link](https://arxiv.org/abs/1710.09767) to the original paper\n",
    "\n",
    "Following the OpenAI implementation \n",
    "\n",
    "* but we don't use MPI for clearer understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the modules .... \n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import gym.spaces\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# OpenAI baselines implementation\n",
    "from distributions import make_pdtype\n",
    "import tf_util as U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = \"/tmp/mlsh_train\"\n",
    "CKPT_DIR = \"/tmp/mlsh_ckpt\"\n",
    "NUM_TIMESTEPS = 1e9\n",
    "M_SEED = 1401\n",
    "\n",
    "# NOTE: CHANGE THE BELOW AS NEEDED\n",
    "NUM_GPU = 0  \n",
    "GLOBAL_STEP_DEVICE = '/cpu:0'\n",
    "DEVICE_PREFIX = '/device:' # probably only used when GPU is needed\n",
    "LOG_DEVICE_PLACEMENT = True\n",
    "gym_env_name = 'CartPole-v1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Hierarchical RL specific\n",
    "num_subpolicies = 2 # How do we decide number of subpolicy ? \n",
    "pri_duration = 1000 # How do we decide duration number ?\n",
    "num_rollouts = 2000 # How do we decide number of rollouts ?\n",
    "num_batches = 15\n",
    "warmup_time = 20 \n",
    "train_time = 30\n",
    "replay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_config=tf.ConfigProto(log_device_placement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/networks.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we are going to implement the policy for our master action\n",
    "\n",
    "Each policy parameters will be optimized based on [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    \"\"\"\n",
    "    At the moment only used MLP policy. (can switch to cnn when we need to)\n",
    "    Would love to try a bayseian policy in here. (whats the benefit?)\n",
    "    \n",
    "    Args: \n",
    "        name - for the tf variable scope\n",
    "        observation - tf placeholder observation tensor\n",
    "        action_space - gym env action_space for the particular action\n",
    "        hidden_size - fc layer hidden units\n",
    "        num_hidden_layers - layers\n",
    "        num_subpolicies - specify sub policies number \n",
    "        gaussian_fixed_var\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 name, \n",
    "                 observation, \n",
    "                 action_space,\n",
    "                 hidden_size, # Fully-connected Layer hidden-layer units\n",
    "                 num_hidden_layers,\n",
    "                 num_subpolicies,\n",
    "                 gaussian_fixed_var=True\n",
    "                ):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_subpolicies = num_subpolicies\n",
    "        self.gaussian_fixed_var = gaussian_fixed_var\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self.namescope=tf.get_variable_scope().name\n",
    "            # NOTE: in the original code, \n",
    "            # they normalize the observation to zero mean however why calculate running mean and running std.\n",
    "            \n",
    "            # build network for value function\n",
    "            last_output = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_output = tf.layers.dense(last_output, \n",
    "                                              hidden_size, \n",
    "                                              name='vfn_fc%i'%(i+1), \n",
    "                                              activation=tf.nn.tanh)\n",
    "            # estimate expected values\n",
    "            self.value_pred = tf.layers.dense(last_output, 1, name='vfn_final')\n",
    "            \n",
    "            # build network for master policy to optimize against\n",
    "            # which subpolicy to pick, as master works by choosing subpolicy\n",
    "            last_output = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_output = tf.layers.dense(last_output, \n",
    "                                              hidden_size, \n",
    "                                              name='master_%i'%(i+1), \n",
    "                                              activation=tf.nn.tanh)\n",
    "            # pick subpolicy\n",
    "            self.policy_selector_prob = tf.layers.dense(last_output, \n",
    "                                                        num_subpolicies, \n",
    "                                                        name='master_final')\n",
    "            # make probability distribution\n",
    "            pdtype = make_pdtype(action_space)\n",
    "            self.pdtype = pdtype\n",
    "            self.pd = pdtype.pdfromflat(self.policy_selector_prob)\n",
    "            \n",
    "        # sample actions\n",
    "        stochastic = tf.placeholder(dtype=tf.bool, shape=())\n",
    "        action = tf.cond(stochastic, lambda: self.pd.sample(), lambda: self.pd.mode())\n",
    "        # i.e. function([placeholders], [outputs])\n",
    "        self._act = U.function([stochastic, observation], \n",
    "                               [action, self.value_pred])\n",
    "            \n",
    "        # debug\n",
    "        self._debug = U.function([stochastic, observation], \n",
    "                                 [action, self.policy_selector_prob])\n",
    "        \n",
    "    def act(self, stochastic, observation):\n",
    "        # no need for observation ?\n",
    "        act1, v_pred1 = self._act(stochastic, observation[None])\n",
    "        return act1[0], v_pred1[0]\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.VARIABLES, self.namescope)\n",
    "    \n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.namescope)\n",
    "    \n",
    "    def reset(self):\n",
    "        with tf.variable_scope(self.namescope, reuse=True):\n",
    "            trainable_vars = self.get_trainable_variables()\n",
    "            initalizer = tf.variables_initializer(trainable_vars)\n",
    "            tf.get_default_session().run(initalizer)\n",
    "    \n",
    "    # debug\n",
    "    \n",
    "    def debug(self, stochastic, observation):\n",
    "        \"\"\"\n",
    "        check which selection we have got\n",
    "        \"\"\"\n",
    "        _, selection = self._debug(stochastic, observation[None])\n",
    "        return selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPolicy(object):\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 observation,\n",
    "                 action_space,\n",
    "                 hidden_size,\n",
    "                 num_hidden_layers,\n",
    "                 gaussian_fixed_var=True):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.gaussian_fixed_var = gaussian_fixed_var\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.namescope = tf.get_variable_scope().name\n",
    "            \n",
    "            # NOTE: in the original code, \n",
    "            # they normalize the observation to zero mean \n",
    "            # however why calculate running mean and running std.\n",
    "            \n",
    "            #value function\n",
    "            last_out = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_out = tf.layers.dense(last_out, hidden_size, activation=tf.nn.tanh, name='vfn_%i'%(i+1))\n",
    "            self.value_pred = tf.layers.dense(last_out, 1, name='vfn_final')\n",
    "            \n",
    "            # sub policy fn\n",
    "            pdtype = make_pdtype(action_space)\n",
    "            paramshape = pdtype.param_shape()[0]\n",
    "            self.pdtype = pdtype\n",
    "            last_out = observation\n",
    "            for i in range(num_hidden_layers):\n",
    "                last_out = tf.layers.dense(last_out, \n",
    "                                           hidden_size, \n",
    "                                           activation=tf.nn.tanh, \n",
    "                                           name='policy_%i'%(i+1))\n",
    "            if gaussian_fixed_var and isinstance(action_space, gym.spaces.Box):\n",
    "                mean = tf.layers.dense(last_out, \n",
    "                                       paramshape//2, \n",
    "                                       name='policy_final')\n",
    "                logstd = tf.get_variable(name='logstd', \n",
    "                                         shape=[1, paramshape//2], \n",
    "                                         initializer=tf.zeros_initializer())\n",
    "                pdparam = tf.concat([mean, mean * 0.0 + logstd], axis=1)\n",
    "            else:\n",
    "                pdparam = tf.layers.dense(last_out, \n",
    "                                          paramshape, \n",
    "                                          name='policy_final')\n",
    "        \n",
    "            self.pd = pdtype.pdfromflat(pdparam)\n",
    "            \n",
    "            #sample actions\n",
    "            stochastic = tf.placeholder(dtype=tf.bool, shape=())\n",
    "            action = tf.cond(stochastic, lambda: self.pd.sample(), lambda: self.pd.mode())\n",
    "            # i.e. function([placeholders], [outputs])\n",
    "            self._act = U.function([stochastic, observation], \n",
    "                                   [action, self.value_pred])\n",
    "            \n",
    "    def act(self, stochastic, observation):\n",
    "        act1, v_pred1 = self._act(stochastic, observation[None])\n",
    "        return act1[0], v_pred1[0]\n",
    "    def get_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.VARIABLES, self.namescope)\n",
    "    def get_trainable_variables(self):\n",
    "        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.namescope)\n",
    "    def reset(self):\n",
    "        with tf.variable_scope(self.namescope, reuse=True):\n",
    "            train_vars = self.get_trainable_variables()\n",
    "            initializer = tf.variables_initializer(train_vars)\n",
    "            tf.get_default_session().run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 master_pol,\n",
    "                 master_oldpol,\n",
    "                 subpolicies,\n",
    "                 old_subpolicies,\n",
    "                 clip_param=0.2,\n",
    "                 entropy_coeff=0,\n",
    "                 optim_epochs=10,\n",
    "                 optim_stepsize=3e-4,\n",
    "                 optim_batchsize=64):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(LOG_DIR):\n",
    "    tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "tf.gfile.MakeDirs(LOG_DIR)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session(config=tf_config) as sess:\n",
    "    env = gym.make(gym_env_name) \n",
    "    observ_space = env.observation_space\n",
    "    action_space = env.action_space\n",
    "    \n",
    "    observ_holder = tf.placeholder(dtype=tf.float32, \n",
    "                                   name='observ', \n",
    "                                   shape=[None, observ_space.shape[0]])\n",
    "    \n",
    "    # PPO master policies\n",
    "    policy = Policy(name='master_policy', \n",
    "                    observation=observ_holder, \n",
    "                    action_space=action_space, \n",
    "                    hidden_size=32, \n",
    "                    num_hidden_layers=2,\n",
    "                    num_subpolicies=num_subpolicies)\n",
    "    old_policy = Policy(name='old_master_policy',\n",
    "                        observation=observ_holder,\n",
    "                        action_space=action_space,\n",
    "                        hidden_size=32,\n",
    "                        num_hidden_layers=2,\n",
    "                        num_subpolicies=num_subpolicies)\n",
    "    \n",
    "    # PPO subpolicies\n",
    "    subpolicies = []\n",
    "    old_subpolicies = []\n",
    "    for i in range(num_subpolicies):\n",
    "        subpolicies.append(SubPolicy(name='subpolicy_%i' % i,\n",
    "                                      observation=observ_holder,\n",
    "                                      action_space=action_space,\n",
    "                                      hidden_size=32,\n",
    "                                      num_hidden_layers=2))\n",
    "        old_subpolicies.append(SubPolicy(name='old_subpolicy_%i' % i, \n",
    "                                         observation=observ_holder,\n",
    "                                         action_space=action_space,\n",
    "                                         hidden_size=32,\n",
    "                                         num_hidden_layers=2))\n",
    "    \n",
    "    # create learner (i.e. agent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
